
# Copiloto de Escrita com Mem√≥ria (vLLM + Chroma + FastAPI + Streamlit)

Um ambiente completo para escrever livros com aux√≠lio de IA, mem√≥ria de cap√≠tulos (RAG) e ferramentas de planejamento criativo.  
Arquitetura: **vLLM (modelo local) + FastAPI (backend) + ChromaDB (vetores) + Streamlit (UI)** ‚Äî tudo orquestrado com **Docker Compose**.

> **Portas padr√£o deste projeto**  
> - **UI (Streamlit):** `http://localhost:8501`  
> - **API (FastAPI):** `http://localhost:8010`  
> - **vLLM (OpenAI API compat√≠vel):** `http://localhost:8015`  ‚Üê *usamos 8015 porque 8000 j√° estava ocupada*  
> - **ChromaDB (HTTP):** `http://localhost:8001`

---

## üß≠ Sum√°rio
- [Vis√£o Geral](#-vis√£o-geral)
- [Arquitetura e Servi√ßos](#-arquitetura-e-servi√ßos)
- [Pr√©-requisitos no Windows (WSL + GPU + Docker)](#-pr√©-requisitos-no-windows-wsl--gpu--docker)
  - [1) Ativar WSL2 e instalar Ubuntu](#1-ativar-wsl2-e-instalar-ubuntu)
  - [2) Instalar drivers NVIDIA (Windows e WSL)](#2-instalar-drivers-nvidia-windows-e-wsl)
  - [3) Instalar Docker Desktop e habilitar WSL2 + GPU](#3-instalar-docker-desktop-e-habilitar-wsl2--gpu)
  - [4) Testar GPU no Docker/WSL](#4-testar-gpu-no-dockerwsl)
- [Instala√ß√£o e Primeira Execu√ß√£o](#-instala√ß√£o-e-primeira-execu√ß√£o)
- [Configura√ß√£o (.env)](#-configura√ß√£o-env)
- [Como usar (UI)](#-como-usar-ui)
- [Refer√™ncia r√°pida da API](#-refer√™ncia-r√°pida-da-api)
- [Trocar de modelo (vLLM)](#-trocar-de-modelo-vllm)
- [Dicas de mem√≥ria (ChromaDB)](#-dicas-de-mem√≥ria-chromadb)
- [Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)
- [FAQ: Tokens, custos e auto-hospedagem](#-faq-tokens-custos-e-auto-hospedagem)
- [Licen√ßa](#-licen√ßa)

---

## üöÄ Vis√£o Geral

O **Copiloto de Escrita** ajuda voc√™ a:
- Escrever cap√≠tulos com um **LLM local compat√≠vel com OpenAI** (via vLLM).
- Salvar cap√≠tulos em disco (`/data/chapters`) e **extrair metadados** (g√™nero, tom, personagens etc).
- **Mem√≥ria** dos cap√≠tulos com **ChromaDB** para consultas de continuidade (RAG).
- Um **UI moderno** (Streamlit) com:
  - Gerenciamento de livros e cap√≠tulos (criar, carregar, sobrescrever).
  - **Perguntas para o copiloto** usando mem√≥ria opcional.
  - **Gerar ideias** e **expandir** em cenas novas (e opcionalmente salvar como cap√≠tulo).
  - **Chroma Explorer** (status, cole√ß√µes, documentos) e a√ß√µes de manuten√ß√£o.

---

## üß© Arquitetura e Servi√ßos

Todos os servi√ßos sobem via Docker Compose:

- **vLLM** (`vllm`) ‚Äî exposto em `localhost:8015` (OpenAI API).  
  Por padr√£o servimos `Qwen/Qwen2.5-14B-Instruct` com `--served-model-name book-llm`.
- **ChromaDB** (`chroma`) ‚Äî exposto em `localhost:8001` (HTTP v2). Persist√™ncia em `./data/chroma`.
- **API** (`book-api`) ‚Äî FastAPI em `localhost:8010`. Fala com `vllm:8000` e `chroma:8000` internamente.
- **UI** (`book-ui`) ‚Äî Streamlit em `localhost:8501`. Fala com `book-api`.

Persist√™ncia local:
```
./data/
  chapters/        # arquivos .md de cap√≠tulos, sugest√µes e cr√≠ticas
  chroma/          # dados do banco vetorial
```

---

## ü™ü Pr√©-requisitos no Windows (WSL + GPU + Docker)

### 1) Ativar WSL2 e instalar Ubuntu
No **PowerShell (Admin)**:
```powershell
wsl --install -d Ubuntu
wsl --set-default-version 2
wsl --status
```
Reinicie se solicitado e **abra o Ubuntu** para concluir a configura√ß√£o de usu√°rio.

### 2) Instalar drivers NVIDIA (Windows e WSL)
- **Windows:** instale o **NVIDIA Game Ready/Studio Driver** mais recente.  
- **WSL CUDA:** instale o pacote de suporte NVIDIA para WSL (o Docker Desktop usar√° o runtime de GPU).  
Verifique no Ubuntu (WSL):
```bash
nvidia-smi
```

### 3) Instalar Docker Desktop e habilitar WSL2 + GPU
- Instale o **Docker Desktop para Windows**.
- Settings ‚Üí **General**: ‚ÄúUse the WSL 2 based engine‚Äù ‚úÖ  
- Settings ‚Üí **Resources ‚Üí WSL Integration**: habilite sua distro Ubuntu ‚úÖ  
- Settings ‚Üí **Resources ‚Üí GPU**: habilite ‚ÄúUse WSL GPU‚Äù / NVIDIA runtime (se dispon√≠vel).

### 4) Testar GPU no Docker/WSL
No Ubuntu/WSL:
```bash
docker run --rm --gpus all nvidia/cuda:12.2.2-base-ubuntu22.04 nvidia-smi
```

Se aparecer a tabela de GPUs, est√° tudo OK.

---

## üì¶ Instala√ß√£o e Primeira Execu√ß√£o

1) **Clone o projeto** (ou copie os arquivos) no Windows (em uma pasta sem acentos/espa√ßos).  
2) Crie a estrutura de dados:
```bash
mkdir -p data/chapters data/chroma
```
3) Crie o arquivo **`.env`** (veja o exemplo abaixo).  
4) **Suba os servi√ßos**:
```bash
docker compose up -d --build
```
> A primeira execu√ß√£o pode demorar pois o vLLM far√° download do modelo (v√°rios GB).

5) **Verifique**:
- UI: `http://localhost:8501`
- API health: `curl http://localhost:8010/health`
- API ready: `curl http://localhost:8010/ready`
- vLLM models: `curl http://localhost:8015/v1/models`
- Chroma heartbeat: `curl http://localhost:8001/api/v2/heartbeat`

---

## üîß Configura√ß√£o (.env)

Crie um arquivo `.env` na raiz:

```dotenv
# Chave fake/local (vLLM aceita qualquer string)
OPENAI_API_KEY=sk-local-anything

# O nome SERVIDO pelo vLLM (veja --served-model-name no compose)
OPENAI_MODEL=book-llm

# Internos da API
CHROMA_HOST=chroma
CHROMA_PORT=8000
DATA_DIR=/data
```

> A UI e a API est√£o configuradas para falar com `vllm:8000` internamente. Externamente, expomos **8015** para testes.

---

## üñ•Ô∏è Como usar (UI)

Acesse `http://localhost:8501`.

1. **Gerenciamento de Livros (sidebar)**
   - Crie um livro (gera um `books/<id>.json`).
   - Selecione um livro existente (detectado por metadados ou pelos arquivos em `/data/chapters`).

2. **Editor de Cap√≠tulo**
   - Edite **T√≠tulo** e **Texto**.
   - **Salvar Cap√≠tulo**:
     - Se **‚ÄúSobrescrever cap√≠tulo carregado‚Äù** estiver **ligado** e um cap√≠tulo estiver carregado, a UI far√° **PUT** para `/chapter/update` (mesmo arquivo).  
     - Se estiver **desligado**, far√° **POST** para `/chapter/save` (novo arquivo).
   - **Gerar Sugest√µes**: chama `/suggest` e salva `sugest_<titulo>__<id>.md`.
   - **Analisar Coer√™ncia**: chama `/critique` e salva `critica_<titulo>__<id>.md`.

3. **Copiloto Livre & Ideias**
   - **Perguntar ao Copiloto**: `/ask` com ou sem mem√≥ria (Top-K).
   - **Gerar Ideias**: `/ideate` retorna lista JSON de ideias (title, logline etc.).
   - **Escrever a partir da Ideia ou Cap√≠tulo**: `/expand` cria uma cena coerente; opcionalmente **salva como cap√≠tulo**.

4. **ChromaDB Explorer**
   - Ver **status**, **cole√ß√µes** e **documentos** via API.
   - A√ß√µes de manuten√ß√£o: **limpar cole√ß√µes** e **reindexar cap√≠tulos do disco**.

> Todos os cap√≠tulos s√£o salvos em `/data/chapters` como `bookId__chapterId.md` (e sugest√µes/cr√≠ticas com prefixos `sugest_`/`critica_`).

---

## üìö Refer√™ncia r√°pida da API

- `GET /health` ‚Äî health b√°sico.  
- `GET /ready` ‚Äî verifica vLLM e Chroma.  
- `POST /test-llm` ‚Äî ping no modelo.

### Cap√≠tulos
- `POST /chapter/save` ‚Äî cria novo cap√≠tulo.  
  Body: `{"book_id","title","text"}`  
- `PUT /chapter/update` ‚Äî **sobrescreve** cap√≠tulo existente.  
  Body: `{"book_id","chapter_id","title?","text?"}`

### Metadados
- `GET /metadata/book/{book_id}` ‚Äî lista documentos do `book_id` na cole√ß√£o `book_memory`.  
- `POST /metadata/extract` ‚Äî extrai metadados do texto enviado.

### Roteirista/Editor
- `POST /suggest` ‚Äî 3‚Äì5 caminhos narrativos + cena amostra.  
- `POST /critique` ‚Äî cr√≠tica de coer√™ncia/continuidade.

### Copiloto Livre & Ideias
- `POST /ask` ‚Äî pergunta livre com mem√≥ria opcional.  
- `POST /ideate` ‚Äî ideias em JSON.  
- `POST /expand` ‚Äî cena a partir de ideia/cap√≠tulo (+ salvar).

### ChromaDB (admin)
- `GET /chroma/status` ‚Äî status do Chroma.  
- `GET /chroma/collections` ‚Äî listas cole√ß√µes.  
- `GET /chroma/collection/{name}` ‚Äî documentos de uma cole√ß√£o.  
- `DELETE /chroma/clear` ‚Äî **apaga tudo**.  
- `POST /chroma/vectorize-existing` ‚Äî indexa `/data/chapters` atuais.

> **Opcional**: se voc√™ adicionou `DELETE /chroma/book/{book_id}`, a UI consegue limpar apenas a mem√≥ria do livro selecionado.

---

## üîÅ Trocar de modelo (vLLM)

O vLLM aceita qualquer modelo do Hugging Face Hub compat√≠vel com gera√ß√£o. Dois presets √∫teis para GPU total ~48GB:

### A) Qwen 2.5 14B Instruct (bom equil√≠brio qualidade/VRAM)
```yaml
services:
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model Qwen/Qwen2.5-14B-Instruct
      --dtype auto
      --max-model-len 16384
      --gpu-memory-utilization 0.92
      --tensor-parallel-size 2
      --served-model-name book-llm
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 2
    ports:
      - "8015:8000"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
```

### B) Phi-3.5 Mini Instruct (leve/r√°pido, √≥timo para testes)
```yaml
services:
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model microsoft/Phi-3.5-mini-instruct
      --dtype auto
      --max-model-len 65536
      --gpu-memory-utilization 0.85
      --tensor-parallel-size 1
      --served-model-name book-llm
    ports:
      - "8015:8000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 1
    volumes:
      - huggingface_cache:/root/.cache/huggingface
```

> Ajuste `--max-model-len` / `--tensor-parallel-size` se faltar VRAM. O nome **`book-llm`** deve bater com `OPENAI_MODEL` na API.

---

## üß† Dicas de mem√≥ria (ChromaDB)

- A cole√ß√£o padr√£o √© **`book_memory`**.  
- Cada cap√≠tulo gera **dois documentos**:  
  - `book_id:chapter_id:summary` (texto de resumo estruturado)  
  - `book_id:chapter_id:full` (texto completo)  
- Use a UI para:
  - **Vectorizar cap√≠tulos antigos** (`/chroma/vectorize-existing`).
  - **Inspecionar documentos** da cole√ß√£o.  
  - **Limpar** (`/chroma/clear`) em caso de bagun√ßa.

---

## üõ†Ô∏è Solu√ß√£o de Problemas

### vLLM demora/repete logs de ‚Äúcarregando shards‚Äù
Normal ‚Äî aguardando download/carregamento. A UI mostra um **spinner de readiness** e verifica `vLLM`/`Chroma`/`API` periodicamente.

### `curl http://localhost:8015/v1/models` falha
- Verifique **Docker Desktop** e se a porta **8015** n√£o est√° ocupada.
- `docker logs vllm --tail=200` para ver o progresso do modelo.
- Ajuste modelo no compose (veja se√ß√£o de modelos).

### `pip Read timed out` durante build da imagem `api`
- O `Dockerfile` j√° usa `--timeout 300 --retries 3`.  
- Se insistir, tente mudar de rede, usar um mirror de PyPI ou `docker build` novamente.

### Chroma: `405 Method Not Allowed` em `/tenants`
- Use **endpoints v2** (`/api/v2/heartbeat`) e o cliente HTTP com `tenant/database` se necess√°rio.  
- No projeto usamos rotas v2 e o `HttpClient` do `chromadb`.

### ‚ÄúContainer unhealthy‚Äù / ‚Äúdependency failed to start‚Äù
- Veja logs: `docker compose logs --tail=200` e corrija o servi√ßo que est√° falhando (geralmente `vllm` ou `api`).

### GPU OOM (falta VRAM)
- Reduza `--max-model-len` ou troque para um modelo menor (Phi-3.5 mini).  
- Diminua `--gpu-memory-utilization` ou `--tensor-parallel-size`.

### Portas em uso
- Se `8000` estiver ocupada, continuamos com **8015:8000**. Altere se precisar e **atualize o README/UI** conforme.

---

## ‚ùì FAQ: Tokens, custos e auto-hospedagem

- **Por que ‚Äúpagar por token‚Äù?** Porque o custo operacional de um LLM cresce com o **tamanho do contexto + gera√ß√£o**. Cobrar por token √© proporcional ao uso real.  
- **Rodando localmente com vLLM** voc√™ **n√£o paga tokens** ‚Äî seu custo √© **hardware + energia**.  
- Modelos como **`microsoft/Phi-3.5-mini-instruct`** s√£o **gratuitos para rodar localmente** (licen√ßas permissivas), por√©m **se usar API de terceiros**, haver√° cobran√ßa por token.  
- Para **SaaS**, voc√™ pode oferecer **bring-your-own-key (BYOK)** ou rodar **modelos pr√≥prios** (custos de GPU).

---

## üìÑ Licen√ßa

Defina a licen√ßa do seu projeto aqui (MIT, Apache-2.0, etc.).

---

**Pronto!** Suba com:
```bash
docker compose up -d --build
```

A UI em `http://localhost:8501` vai exibir o status de readiness e liberar a edi√ß√£o quando tudo estiver no ar.
