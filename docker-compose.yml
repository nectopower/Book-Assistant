# version: "3.9"  # (opcional) remova para nÃ£o aparecer o warning

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    command: >
      --model Qwen/Qwen2.5-14B-Instruct
      --dtype auto
      --max-model-len 131072
      --tensor-parallel-size 2
      --gpu-memory-utilization 0.9
      --max-num-batched-tokens 2048
      --served-model-name book-llm
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    ports:
      - "8015:8000"
    shm_size: "16gb"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 2
    volumes:
      - huggingface_cache:/root/.cache/huggingface

  chroma:
    image: chromadb/chroma:latest
    container_name: chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    volumes:
      - ./data/chroma:/chroma/chroma
    ports:
      - "8001:8000"

  api:
    build:
      context: ./api
    container_name: book-api
    env_file:
      - .env
    environment:
      - OPENAI_API_BASE=http://vllm:8000/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY:-sk-local}
      - OPENAI_MODEL=${OPENAI_MODEL:-book-llm}
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - DATA_DIR=/data
    volumes:
      - ./data:/data
    ports:
      - "8010:8010"
    depends_on:
      chroma:
        condition: service_started

  ui:
    build:
      context: ./ui
    container_name: book-ui
    env_file:
      - .env
    environment:
      - API_BASE=http://api:8010
    volumes:
      - ./data:/data
    ports:
      - "8501:8501"
    depends_on:
      api:
        condition: service_started

volumes:
  huggingface_cache:
