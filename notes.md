# Book Assistant - Guia de Uso

## üì± 1. Streamlit UI ‚Äî "O App que Voc√™ Usa"

**URL:** http://localhost:8501

**Para qu√™:** √â a interface gr√°fica onde voc√™ escreve/cola o cap√≠tulo, salva, e solicita funcionalidades.

### Funcionalidades Principais:
- **üíæ Salvar cap√≠tulo** ‚Üí Gera e armazena o resumo estruturado + indexa no Chroma
- **üí° Sugerir pr√≥ximos passos** ‚Üí Ideias de enredo coerentes com a mem√≥ria
- **üß≠ Cr√≠tica de coer√™ncia** ‚Üí Checklist de continuidade/contradi√ß√µes

**Quando acessar:** No dia a dia da escrita. √â o que voc√™ abre no navegador e trabalha normalmente.

---

## üß† 2. API (FastAPI) ‚Äî "O C√©rebro de Orquestra√ß√£o/Mem√≥ria"

**URL:** http://localhost:8010/health (s√≥ para conferir que est√° ok)

**Para qu√™:** Endpoints REST que a UI usa por tr√°s dos panos. Voc√™ pode cham√°-los direto se quiser automatizar coisas, scripts, integra√ß√£o com outros apps.

### Endpoints Principais:

- **POST** `/chapter/save` ‚Äî Salva cap√≠tulo + cria resumo + indexa na mem√≥ria
- **POST** `/suggest` ‚Äî Gera sugest√µes para o pr√≥ximo passo com base na mem√≥ria
- **POST** `/critique` ‚Äî Faz revis√£o de coer√™ncia

### Quando Acessar:

- **Para testar/depurar:** Ver se a API est√° de p√© (`/health`)
- **Para integra√ß√µes:** Chamar endpoints a partir de outro sistema

### Exemplo R√°pido (PowerShell/CMD):

```bash
curl -X POST http://localhost:8010/suggest ^
  -H "Content-Type: application/json" ^
  -d "{\"book_id\":\"meu-livro\",\"current_chapter_title\":\"Cap 7\",\"current_chapter_text\":\"texto...\",\"k\":8}"
```

---

## ü§ñ 3. vLLM (OpenAI API) ‚Äî "O Servidor do Modelo"

**URL base:** http://localhost:8000/v1

**Para qu√™:** Exp√µe o modelo (Qwen 32B) em API compat√≠vel com OpenAI. A API e a UI falam com ele. Voc√™ tamb√©m pode usar este endpoint direto com qualquer cliente OpenAI (SDK) local.

### Quando Acessar:

- **Para testar o modelo cru** (sem RAG/mem√≥ria) ‚Äî √∫til para diagn√≥stico de qualidade
- **Para usar o modelo em outros projetos** que falem OpenAI

### Testes √öteis:

#### Listar Modelos:
```bash
curl http://localhost:8000/v1/models
```

#### Chat Simples:
```bash
curl http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -H "Authorization: Bearer sk-local-anything" ^
  -d "{\"model\":\"book-llm\",\"messages\":[{\"role\":\"user\",\"content\":\"Escreva um par√°grafo criativo em PT-BR.\"}]}"
```

> **Nota:** `book-llm` √© o nome servido (definido no `--served-model-name`).

---

## üîÑ Fluxo Pr√°tico (Resumo)

1. **Suba os servi√ßos** com `docker compose up -d --build`
2. **Abra a UI** (:8501) e trabalhe nos cap√≠tulos
3. **Se algo parecer estranho:**
   - Cheque a API em `:8010/health`
   - Teste o vLLM em `:8000/v1/models` e um chat simples (acima)
   - Este comando roda o modelo setado > --model Qwen/Qwen2.5-32B-Instruct

---

### Subir modo r√°pido (14B):
```bash 
docker compose -f docker-compose.yml -f docker-compose.14b.yml up -d --build vllm
```

### Subir alta qualidade (32B):

```bash 
docker compose -f docker-compose.yml -f docker-compose.32b.yml up -d --build vllm
```

### Subir Interface gr√°fica:

```bash 
docker compose -f docker-compose.yml -f docker-compose.14b.yml up -d --build chroma api ui

```

## üí° Dicas e Solu√ß√£o de Problemas

### Primeira Execu√ß√£o
- O modelo baixa os pesos no servi√ßo vllm na primeira execu√ß√£o

### Logs (em outra janela/terminal):
```bash
docker compose logs -f vllm
docker compose logs -f api
docker compose logs -f ui
```

### Recarregar Depois de Alterar `docker-compose.yml`:
```bash
docker compose up -d --build
```

### Checar Uso das Duas GPUs:
```bash
docker exec -it vllm nvidia-smi
```

> **Resultado esperado:** Voc√™ deve ver a 3090 e a 4090 com mem√≥ria alocada quando gerar texto.

### Mudou o Modelo/Janela de Contexto?
Edite o `docker-compose.yml` (bloco vllm, flags `--model`, `--max-model-len`, `--tensor-parallel-size`) e recrie o servi√ßo:

```bash 
docker compose up -d --build vllm
```

### Checar download/carregamento do modelo:
````bash
docker compose logs -f vllm
````

### Testar chat r√°pido:

````bash
curl http://localhost:8000/v1/chat/completions `
  -H "Content-Type: application/json" `
  -H "Authorization: Bearer sk-local-anything" `
  -d "{\"model\":\"book-llm\",\"messages\":[{\"role\":\"user\",\"content\":\"Escreva um par√°grafo criativo em PT-BR.\"}]}"

````

